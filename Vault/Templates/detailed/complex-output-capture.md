---
title: "Output: {{title}}"
prompt: "[[Link to Prompt]]"
model: "Model Name (e.g., GPT-4, Claude 2)"
date_generated: {{date}}
tags: [output, LLM, AI]
---

# Output: {{title}}

## Prompt Information

- **Prompt Title**: [[Link to Prompt]]
- **Prompt Version**: [Version number of the prompt used]
- **Model Used**: {{model}}
- **Date Generated**: {{date}}

## Configuration

- Temperature: [0.0 to 1.0]
- Top P: [0.0 to 1.0]
- Frequency Penalty: [0.0 to 2.0]
- Presence Penalty: [0.0 to 2.0]
- Max Tokens: [Number]

## Raw Output


## Edited Output

[If you've made any edits or refinements to the raw output, include the edited version here]

## Analysis

### Strengths

- [List key strengths or successful aspects of the output]

### Weaknesses

- [List any weaknesses, inaccuracies, or areas for improvement]

### Unexpected Results

- [Note any surprising or unexpected elements in the output]

## Accuracy Assessment

- **Overall Accuracy**: [Rate the overall accuracy, e.g., High/Medium/Low]
- **Factual Errors**: [List any identified factual errors]
- **Logical Consistency**: [Comment on the logical flow and consistency of the output]

## Use Case Evaluation

- **Intended Use**: [Brief description of how this output was intended to be used]
- **Effectiveness**: [Evaluate how well the output meets the intended use]
- **Limitations**: [Note any limitations for practical application]

## Human Intervention

- **Edits Made**: [Describe any edits or corrections made to the output]
- **Reasoning**: [Explain the rationale behind any edits]

## Follow-up Actions

- [ ] Refine prompt based on output
- [ ] Fact-check specific claims
- [ ] Test output in practical application
- [ ] Compare with outputs from different models/settings

## Related Outputs

- [[Link to related output 1]]
- [[Link to related output 2]]

## Notes

[Any additional observations, ideas for improvement, or context that might be useful]
